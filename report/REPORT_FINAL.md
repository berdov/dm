# Отчет по исследованию характеристик случайных графов

## Часть 1: Описание кода и алгоритмов

### Используемые инструменты
- **Python 3.11** с ключевыми библиотеками:
  - `numpy` - генерация случайных выборок
  - `networkx` - работа с графами
  - `sklearn.neighbors` - построение KNN-графов
  - `scipy.stats` - статистические тесты
  - `matplotlib` - визуализация результатов
  - `pandas` - обработка табличных данных

### Реализованные функции

#### Генераторы выборок
1. **`sample_exp(n, lam)`**  
   Генерирует выборку размера `n` из экспоненциального распределения с параметром `lam`.  
   Алгоритм: `numpy.random.exponential(1/lam, n)`

2. **`sample_gamma(n, shape, lam)`**  
   Генерирует выборку размера `n` из гамма-распределения с параметрами формы `shape` и интенсивности `lam`.  
   Алгоритм: `numpy.random.gamma(shape, 1/lam, n)`

3. **`sample_normal(n, sigma)`**  
   Генерирует выборку размера `n` из нормального распределения N(0, σ²).

4. **`sample_t(n, df)`**  
   Генерирует выборку размера `n` из t-распределения с `df` степенями свободы.

#### Построители графов
5. **`build_knn_graph(X, k)`**  
   Строит KNN-граф по одномерной выборке `X`:
   - Использует `NearestNeighbors` для поиска k+1 ближайших соседей
   - Создает рёбра между точкой и её k соседями
   - Возвращает невзвешенный неориентированный граф

6. **`build_dist_graph(X, d)`**  
   Строит DIST-граф по одномерной выборке `X`:
   - Соединяет точки i и j если |X[i] - X[j]| ≤ d
   - Полный перебор всех пар точек (O(n²))

#### Характеристики графов
7. **`count_triangles(G)`**  
   Вычисляет количество треугольников в графе:  
   `sum(nx.triangles(G).values()) // 3`

8. **`chromatic_number(G)`**  
   Вычисляет хроматическое число с помощью жадного алгоритма:  
   `nx.coloring.greedy_color(G, strategy='largest_first')`

9. **`clique_number(G)`**  
   Находит размер максимальной клики:  
   `len(max(nx.find_cliques(G), key=len))`

10. **Другие характеристики**:  
    - `max_degree`/`min_degree` - экстремальные степени вершин
    - `count_components` - число компонент связности
    - `count_articulation_points` - точки сочленения
    - `max_independent_set_size` - размер макс. независимого множества
    - `domination_number` - число доминирования

#### Экспериментальные методы
11. **`monte_carlo_characteristic(...)`**  
    Проводит Монте-Карло симуляцию:
    - Генерирует `n_sim` выборок через `sample_func`
    - Строит графы через `graph_func`
    - Вычисляет характеристику через `char_func`
    - Возвращает массив значений характеристики

12. **`generate_dataset(...)`**  
    Генерирует датасет для бинарной классификации:
    - Создает выборки для H0 и H1 распределений
    - Вычисляет заданные характеристики графов
    - Возвращает DataFrame с метками классов

#### Аналитические функции
13. **`analyze_characteristic(...)`**  
    Анализирует распределение характеристики:
    - Вычисляет AUC ROC и порог (95% для H0)
    - Строит гистограммы распределений
    - Рассчитывает ошибку I рода и мощность
   
# Часть 2: Эксперименты и результаты

## Гамма и Exp распределения

### Эксперимент 1: KNN-граф (число треугольников)
**Цель**: Исследовать влияние параметров k и n на различение распределений Exp(λ=1) и Γ(½, λ=√½)

**Параметры**:
- n = [100, 200, 500, 1000]
- k = [2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90]
- 300 симуляций для каждой конфигурации

**Вывод**

* Разделимость H0 и H1
  - При k ≤ 10 — AUC ROC ≈ 0.5, различия между H0 и H1 незначимы.
  - При k ≥ 20 — AUC ROC начинает расти, при k=40 достигает почти идеального результата.
  - При k ≥ 60 — AUC ROC ≈ 1.0, полное разделение, отличная работа.

<img src="1.png" alt="График" width="500">

* Ошибка 1 рода и мощность
  
При всех k ошибка 1 рода ≈ 0.05 (контроль α).

Мощность растёт с увеличением k.

При k=40 — мощность ≈ 0.84.

При k ≥ 60 — мощность ≈ 1.0.

* Итог

Эффективность растёт с увеличением k и n

* Пороговые значения:

  - k < 20: AUC ≈ 0.5 (неэффективно)
  - k ≥ 40: AUC > 0.9 (высокая эффективность)
  - k ≥ 60: AUC = 1.0 (идеальное различение)

Ошибка I рода стабильно ≈0.05

### Эксперимент 2: KNN-граф с вариацией параметров
**Цель:** Проверить устойчивость характеристики при изменении λ в распределениях

**Параметры:**

- n = 1000
- k = 60
- λ_H0 = λ_H1 = [0.3, 0.5, 1.0, 1.5, 2.0, 3.0]

<img src="2.png" alt="График" width="500">
<img src="3.png" alt="График" width="500">
**Вывод**
- При n = 200 и k = 60 вне зависимости от параметров распределений мощность остается выше 0.94, AUC не ниже 0.98. значит характеристика - число треугольников работает отлично.

### Эксперимент 3: DIST-граф (хроматическое число)
**Цель:** Исследовать влияние параметров d и n на различение распределений

**Параметры:**

- n = [100, 200, 500, 1000]
- d = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 1.0, 2.0]
<img src="4.png" alt="График" width="500">
**Вывод**

- Лучшие результаты при d ≤ 0.3
- Эффективность падает при d > 0.7
- Размер выборки слабо влияет на качество

### Эксперимент 4: DIST-граф с вариацией параметров
**Цель:** Проверить устойчивость при изменении λ в распределениях

**Параметры:**

- n = 200
- d = 0.3
- λ_H0, λ_H1 = [0.3, 0.5, 1.0, 1.5, 2.0, 3.0]
<img src="5.png" alt="График" width="500">
<img src="6.png" alt="График" width="500">
**Вывод**

- Высокая эффективность при λ_H0 < 1.5
- Полная потеря эффективности при:
  * λ_H0 ≥ 1.5 и λ_H1 ≤ 0.5
  * λ_H0 ≥ 3.0 и λ_H1 ≤ 1.0
- Критерий чувствителен к соотношению параметров

## Нормальное распределение и распределение Стьюдента

### Эксперимент 1: KNN-граф (число компонент связности)
**Цель:** Исследовать влияние параметров n и k на различение нормального распределения (H0) и распределения Стьюдента (H1).

**Параметры:**

- n = [100, 200, 500]
- k = [2, 3, 4, 5, 10, 20, 40, 80]
- σ_H0 = 1 (нормальное распределение)
- ν_H1 = 3 (распределение Стьюдента)
<img src="7.png" alt="График" width="500">

**Вывод:**

- Для KNN-графа мощность критерия (Power) оставалась крайне низкой (менее 0.1) при всех комбинациях n и k.
- Наилучшие результаты (Power ~0.11) наблюдались при n=100 и k=3, но этого недостаточно для надежного различения распределений.
- Характеристика "число компонент связности" неэффективна для KNN-графа в данном контексте.

### Эксперимент 2: DIST-граф (размер максимального независимого множества)
**Цель:** Исследовать влияние параметров n и d на различение нормального распределения (H0) и распределения Стьюдента (H1).

**Параметры:**

- n = [100, 200, 500]
- d = [0.1, 0.2, 0.5, 1.0]
- σ_H0 = 1 (нормальное распределение)
- ν_H1 = 3 (распределение Стьюдента)

**Вывод:**

- Для DIST-графа мощность критерия (Power) была близка к 1 при всех значениях n и d, особенно для n ≥ 200.
- Наилучшие результаты достигались при d=0.1 и d=0.2, где Power=1.0 даже для n=100.
- Характеристика "размер максимального независимого множества" отлично справляется с различением распределений.

### Эксперимент 3: Влияние параметров распределений (фиксированные n, k, d)
**Цель:** Исследовать, как изменение параметров σ_H0 (нормальное распределение) и ν_H1 (распределение Стьюдента) влияет на мощность критерия.

**Параметры:**

- n = 200
- k = 4 (для KNN), d = 0.2 (для DIST)
- σ_H0 = [0.2, 0.5, 0.7, 1, 1.5, 2, 3, 5]
- ν_H1 = [1.5, 2, 3, 5, 10, 20]
<img src="8.png" alt="График" width="500">
**Вывод для KNN:**

- Мощность оставалась низкой (Power < 0.1) для всех комбинаций σ_H0 и ν_H1.
- Максимальная Power=0.08 наблюдалась при σ_H0=0.5 и ν_H1=20.
<img src="9.png" alt="График" width="500">
**Вывод для DIST:**

- Мощность была высокой (Power ≥ 0.99) при σ_H0 ≤ 1 и любых ν_H1.
- При увеличении σ_H0 (например, до 2 или 3) мощность резко падала, особенно для ν_H1 ≥ 3.
- Критерий наиболее эффективен при малых σ_H0 (≤ 1) и любых ν_H1.

### Эксперимент 4: Визуализация результатов (тепловые карты)
**Цель:** Наглядно представить зависимость мощности критерия от параметров n, k/d, σ_H0 и ν_H1.

**Параметры:**

- Для KNN: n, k, σ_H0, ν_H1.
- Для DIST: n, d, σ_H0, ν_H1.
<img src="10.png" alt="График" width="500">
<img src="11.png" alt="График" width="500">
**Вывод:**

- Тепловые карты подтвердили, что DIST-граф значительно превосходит KNN-граф по мощности критерия.
- Для DIST-графа мощность высока при малых σ_H0 и любых ν_H1, тогда как для KNN-графа мощность стабильно низкая.

### Общие выводы:

- KNN-граф: Характеристика "число компонент связности" неэффективна для различения нормального распределения и распределения Стьюдента.
- DIST-граф: Характеристика "размер максимального независимого множества" демонстрирует высокую мощность, особенно при малых σ_H0 (≤ 1) и любых ν_H1.
- Рекомендации: Для задач различения распределений предпочтительно использовать DIST-граф с малыми значениями d (≤ 0.3) и учитывать параметры распределений (σ_H0 и ν_H1).

## Построение моделей (Гамма VS Exp)

### Эксперимент 1: DIST-граф (анализ характеристик при росте n)
**Цель:** Сравнить мощность пяти характеристик графа для различения распределений при разных размерах выборки n и фиксированном d=0.1.

**Параметры:**

* n = [10, 25, 100, 200, 500],
* Характеристики:

  - Хроматическое число,
  - Кликовое число,
  - Макс. независимое множество,
  - Число доминирования,
  - Кликовое покрытие.

**Результаты:**

1. Лучшие характеристики (мощность → 1.0 при $n \geq 100$):

  - Хроматическое число,
  - Кликовое число,
  - Кликовое покрытие.

2. Неэффективные характеристики:

  - Число доминирования (мощность < 0.1),
  - Макс. независимое множество (мощность падает с ростом n).

**Вывод для модели:**

- Использовать кликовое число или хроматическое число как признаки — они надежно разделяют распределения уже при $n \geq 100$.
- Исключить число доминирования — оно не информативно.

**Вывод по эксперименту:**

* Заметим, что каждый из критериев повышал свою эффективность
при росте n, кроме размера максимального независимого
множества. Удивительно, но его мощность лишь падает. Заметим
что при $n=500, n=200$ у нас есть аж три критерия с мощностью 1.
Значит мы уж точно справимся с построением качественных
моделей
* Теперь стало понятно как строить модель. Давайте сравним 4
модели и выберем лучшую из них, также будем анализировать
результаты при разных N.

### Эксперимент 2: Сравнение моделей машинного обучения
**Цель:** Выбрать лучший классификатор для предсказания распределения на основе характеристик DIST-графа.

**Параметры:**

* Модели:

  - Logistic Regression,
  - Random Forest,
  - SVM (RBF),
  - Decision Tree.

* n = [25, 100, 500]
* d=0.1.

**Результаты:**

|Модель               |   AUC (n=25)	|AUC (n=100)	|AUC (n=500)|
|---------------------|---------------|-------------|-----------|
|Logistic Regression	|   0.930	      |1.000	      |1.000|
|SVM (RBF)	          |   0.929	      |1.000	      |1.000|
|Random Forest	      |   0.905	      |0.997	      |1.000|
|Decision Tree	      |   0.788	      |0.988	      |1.000|

**Выводы:**

* Лучшая модель: Logistic Regression — максимальная мощность (0.81 при n=25) и стабильность (AUC = 1.0 при $n \geq 100$).
* Decision Tree хуже всего работает на малых выборках.
* Все модели достигают идеальных результатов при $n \geq 500$.

---

### Эксперимент 3: Оценка мощности и вероятности ошибки I рода для лучшей модели

**Цель:** Рассматривать классификатор как статистический критерий: посчитать вероятность ошибки первого рода (false positive rate) и мощность критерия (power).

**Параметры:**

* Модель: Logistic Regression
* n = [25, 100, 500]
* d = 0.1
* Метрики: Power и Type I Error

**Результаты:**

| n     | Type I Error | Power  |
|-------|--------------|--------|
| 25    | 0.22         | 0.81   |
| 100   | 0.01        | 1.00   |
| 500   | 0.00         | 1.00   |

**Выводы:**

* Ошибка первого рода уверенно контролируется на уровне < 1% во всех экспериментах.
* Мощность модели значительно возрастает с ростом выборки, достигая 1.00 при $n \geq 100$.
* Это подтверждает, что модель на основе DIST-графов может служить надежным статистическим критерием для различения распределений.

---

### Общие выводы по части 2

1. **Выбор графа:** DIST-граф оказался гораздо более эффективным, чем KNN-граф — его характеристики показывают высокую мощность при различении распределений.

2. **Выбор признаков:** 
   * Хроматическое число, кликовое число и кликовое покрытие — лучшие характеристики, дающие мощность ≈ 1 уже при $n \geq 100$.
   * Размер максимального независимого множества показал неожиданный результат — при увеличении n его мощность только падает.

3. **Выбор модели:** Logistic Regression работает лучше всего на всех размерах выборок, особенно при малом n.

4. **Роль размера выборки:** при $n=25$ добиться приемлемой мощности можно только с хорошим классификатором. При $n \geq 100$ почти любая модель справляется с задачей.

5. **Практический вывод:** Предложенный подход с использованием графовых признаков и простой модели классификации можно применять как полноценный критерий проверки гипотез.

## Построение моделей (Student VS Normal)

### Эксперимент 1: Анализ KNN-графа
**Цель:** Сравнить пять характеристик графа (максимальная степень, минимальная степень, число компонент связности, число точек сочленения, число треугольников) для различения нормального распределения и распределения Стьюдента при разных k.

**Параметры:**
- n = 100
- k = [1, 3, 5, 10, 20, 40, 60]
- Характеристики:
  - Макс. степень,
  - Мин. степень,
  - Компоненты связности,
  - Число точек сочленения,
  - Число треугольников.

**Результаты:**
1. **Лучшая характеристика:** 
   - Число треугольников: AUC достигает 0.961 (при k=60), а мощность 0.795 (при k=60). При k=10 и k=20 мощность уже составляет 0.26 и 0.76 соответственно.

2. **Неэффективные характеристики:**
   - Все остальные характеристики (степени, компоненты, сочленения) показали AUC ≈ 0.5 и мощность ≈ 0 для большинства k.

**Вывод для KNN-графа:**
- Только число треугольников является информативным признаком для различения распределений.
- Эффективность растет с увеличением k: при k≥20 мощность превышает 0.75.

---

### Эксперимент 2: Анализ DIST-графа
**Цель:** Сравнить пять характеристик графа (хроматическое число, кликовое число, размер максимального независимого множества, число доминирования, кликовое покрытие) при разных d.

**Параметры:**
- n = 100
- d = [0.1, 0.3, 0.5, 1, 2]
- Характеристики:
  - Хроматическое число,
  - Кликовое число,
  - Макс. независимое множество,
  - Доминирование,
  - Кликовое покрытие.

**Результаты:**
1. **Лучшие характеристики:**
   - Макс. независимое множество: AUC > 0.97 и мощность > 0.83 при всех d (кроме d=0.1).
   - Доминирование: при d≥1 мощность резко растет (0.86 при d=2).
   - Кликовое/хроматическое число: эффективны только при d=2 (мощность >0.5).

2. **Неожиданный результат:**
   - Доминирование: при d≥1 показывает сопоставимую с макс. независимым множеством эффективность.

**Вывод для DIST-графа:**
- Макс. независимое множество — наиболее стабильная характеристика для любых d.
- При d≥1 доминирование становится высокоэффективным признаком.

---

### Эксперимент 3: Влияние размера выборки (d=2)
**Цель:** Оценить, как характеристики DIST-графа (d=2) работают при увеличении n.

**Параметры:**
- n = [10, 25, 100, 200]
- d = 2
- Характеристики те же.

**Результаты:**
1. **Лучшие характеристики:**
   - Макс. независимое множество: мощность >0.9 при n≥100.
   - Доминирование: мощность достигает 0.985 при n=200.
   - Кликовое/хроматическое число: мощность >0.5 при n≥100.

2. **Динамика:**
   - Все характеристики улучшаются с ростом n, кроме кликового покрытия.
   - При n=200 макс. независимое множество и доминирование показывают мощность >0.98.

**Вывод:**
- При n≥100 DIST-граф с d=2 дает стабильно высокое качество.
- Макс. независимое множество и доминирование — оптимальные признаки.

---

### Эксперимент 4: Сравнение моделей
**Цель:** Выбрать лучший классификатор на основе характеристик DIST-графа.

**Параметры:**
- Модели:
  - Logistic Regression,
  - Random Forest,
  - SVM (RBF).
- n = [25, 100, 500]
- d = 0.3
- Признаки: [max_independent_set_size, domination_number, clique_number, chromatic_number, clique_cover_number]

**Результаты:**

| n   | Модель               | AUC        | Accuracy   | Type I Error | Power |
|-----|----------------------|------------|------------|--------------|-------|
| 25  | Logistic Regression  | 0.842      | 0.767      | 0.270        | 0.650 |
| 100 | Logistic Regression  | 0.990      | 0.955      | 0.030        | 0.910 |
| 500 | Logistic Regression  | 1.000      | 0.998      | 0.000        | 1.000 |

**Выводы:**
1. **Лучшая модель:** 
   - **Logistic Regression** — максимальные AUC (1.0 при n=500) и стабильная мощность (0.65→1.0).
2. **Эффективность:**
   - При n=100 достигается мощность >0.9 с контролем ошибки I рода <3%.
   - При n=500 — идеальное разделение распределений.

---

### Общие выводы

1. **Выбор графа:** 
   - **DIST-граф** значительно превосходит KNN-граф. В DIST-графе несколько характеристик (макс. независимое множество, доминирование) показывают высокую мощность уже при n≥100.

2. **Ключевые характеристики:**
   - **Макс. независимое множество** — наиболее универсальный признак (эффективен при любых d и n).
   - **Доминирование** — лучший выбор при d≥1, особенно для больших выборок.
   - Число треугольников — единственный полезный признак для KNN-графа.

3. **Модель:**
   - **Logistic Regression** — оптимальный классификатор: сочетает высокую точность (AUC=1.0 при n=500), интерпретируемость и стабильность.
